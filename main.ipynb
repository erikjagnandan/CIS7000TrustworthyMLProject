{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Xqo7pyX-n72M"
      },
      "outputs": [],
      "source": [
        "!pip install mujoco\n",
        "\n",
        "# Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
        "print('Setting environment variable to use GPU rendering:')\n",
        "%env MUJOCO_GL=egl\n",
        "\n",
        "# Check if installation was succesful.\n",
        "try:\n",
        "  print('Checking that the installation succeeded:')\n",
        "  import mujoco\n",
        "  mujoco.MjModel.from_xml_string('<mujoco/>')\n",
        "except Exception as e:\n",
        "  raise e from RuntimeError(\n",
        "      'Something went wrong during installation. Check the shell output above '\n",
        "      'for more information.\\n'\n",
        "      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
        "      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
        "\n",
        "print('Installation successful.')\n",
        "\n",
        "# Other imports and helper functions\n",
        "import time\n",
        "import itertools\n",
        "import numpy as np\n",
        "\n",
        "# Graphics and plotting.\n",
        "print('Installing mediapy:')\n",
        "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
        "!pip install -q mediapy\n",
        "import mediapy as media\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# More legible printing from numpy.\n",
        "np.set_printoptions(precision=3, suppress=True, linewidth=100)\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZEvep1WKE4C",
        "outputId": "80ac362b-f94f-4931-ef7f-2c06a06b867b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.11.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Collecting stable_baselines3\n",
            "  Downloading stable_baselines3-2.3.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.2/182.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.2.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.13.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable_baselines3) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable_baselines3) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable_baselines3\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 stable_baselines3-2.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install stable_baselines3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZBz3dHRDKA5H"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from stable_baselines3 import SAC, TD3, A2C, DQN, PPO\n",
        "import os\n",
        "import argparse\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YclEDV15zHhx"
      },
      "outputs": [],
      "source": [
        "def numerical_grad_of_reward_wrt_obs(obs, model, env, rollout_len=10, delta=1e-4):\n",
        "    grad = np.zeros(obs.shape)\n",
        "    for i in range(obs.shape[0]):\n",
        "        obs_plus = obs.copy()\n",
        "        obs_minus = obs.copy()\n",
        "        obs_plus[i] += delta\n",
        "        obs_minus[i] -= delta\n",
        "\n",
        "        action_plus, _ = model.predict(obs_plus)\n",
        "        base_env = deepcopy(env)\n",
        "        result = env.step(action_plus)\n",
        "        obs_new_plus = result[0]\n",
        "        reward_plus = result[1]\n",
        "        for j in range(rollout_len - 1):\n",
        "            action_plus, _ = model.predict(obs_new_plus)\n",
        "            result = env.step(action_plus)\n",
        "            obs_new_plus = result[0]\n",
        "            reward_plus += result[1]\n",
        "        reward_plus = reward_plus / rollout_len\n",
        "        env = deepcopy(base_env)\n",
        "\n",
        "        action_minus, _ = model.predict(obs_minus)\n",
        "        base_env = deepcopy(env)\n",
        "        result = env.step(action_minus)\n",
        "        obs_new_minus = result[0]\n",
        "        reward_minus = result[1]\n",
        "        for j in range(rollout_len - 1):\n",
        "            action_minus, _ = model.predict(obs_new_minus)\n",
        "            result = env.step(action_minus)\n",
        "            obs_new_minus = result[0]\n",
        "            reward_minus += result[1]\n",
        "        reward_minus = reward_minus / rollout_len\n",
        "        env = deepcopy(base_env)\n",
        "\n",
        "        grad[i] = (reward_plus - reward_minus) / (2*delta)\n",
        "\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OFFUjNQR0WzG"
      },
      "outputs": [],
      "source": [
        "def train(env_name, num_epochs, timesteps):\n",
        "    model = PPO('MlpPolicy', env_name, verbose=1, device='cpu', tensorboard_log=log_dir)\n",
        "\n",
        "    for i in range(num_epochs):\n",
        "\n",
        "        model.learn(total_timesteps=timesteps, reset_num_timesteps=False)\n",
        "        model.save(f\"{model_dir}/{'PPO'}_{timesteps * (i+1)}\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = \"HalfCheetah-v4\"\n",
        "model_dir = \"models_\" + env_name\n",
        "log_dir = \"logs_\" + env_name\n",
        "num_epochs = 20\n",
        "num_iters = 25000\n",
        "num_test_steps = 100\n",
        "index = num_epochs * num_iters\n",
        "model = train(env_name, num_epochs, num_iters)"
      ],
      "metadata": {
        "id": "ItHNiJczMwe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wNWhn8bQ0BKO"
      },
      "outputs": [],
      "source": [
        "def projected_gradient_descent(obs, model, env, v, num_attack_iters=10, rollout_len=10, epsilon=0.05, alpha=3e-7, delta=1e-4):\n",
        "    original_obs = obs.copy()\n",
        "    for i in range(num_attack_iters):\n",
        "        grad = numerical_grad_of_reward_wrt_obs(obs, model, env, rollout_len, delta)\n",
        "        obs = obs - alpha * grad\n",
        "        clipped_obs = np.clip(obs, original_obs - v*epsilon, original_obs + v*epsilon)\n",
        "    return clipped_obs\n",
        "\n",
        "def fgsm(obs,  model, env, v, rollout_len=10, epsilon=0.05, delta=1e-4):\n",
        "    signed_grad = np.sign(numerical_grad_of_reward_wrt_obs(obs, model, env, rollout_len, delta))\n",
        "    scaled_grad = signed_grad * v * epsilon\n",
        "    obs = obs - scaled_grad\n",
        "    return obs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bjiv1R670ErN"
      },
      "outputs": [],
      "source": [
        "def test(model, env_name, index, max_steps, randomized_smoothing=False, lamda=0.001, Sigma=None, num_smoothed_samples=10, adversarial_attack=None,\n",
        "         adversarial_mode=None, v=None, rollout_len=10, epsilon=0.05, alpha=3e-7, delta=1e-4, num_attack_iters=10):\n",
        "\n",
        "    env = gym.make(env_name)\n",
        "    obs = env.reset()[0]\n",
        "    reward_sum = 0\n",
        "    num_steps = 0\n",
        "    model.set_env(env)\n",
        "\n",
        "    if v is None:\n",
        "        v = np.ones(obs.shape[0])\n",
        "    if Sigma is None:\n",
        "        Sigma = np.eye(obs.shape[0])\n",
        "\n",
        "    if adversarial_attack == \"Start State\":\n",
        "        if adversarial_mode == \"PGD\":\n",
        "            adversarial_obs = projected_gradient_descent(obs, model, env, v, num_attack_iters, rollout_len, epsilon, alpha, delta)\n",
        "        else:\n",
        "            adversarial_obs = fgsm(obs, model, env, v, rollout_len, epsilon, delta)\n",
        "        obs = env.reset()[0]\n",
        "        obs = adversarial_obs.copy()\n",
        "\n",
        "    terminated = False\n",
        "    while not terminated:\n",
        "        obs_predict = obs.copy()\n",
        "\n",
        "        if adversarial_attack == \"Observation Corruption\":\n",
        "            if adversarial_mode == \"PGD\":\n",
        "                adversarial_obs = projected_gradient_descent(obs_predict, model, env, v, num_attack_iters, rollout_len, epsilon, alpha, delta)\n",
        "            else:\n",
        "                adversarial_obs = fgsm(obs_predict, model, env, v, rollout_len, epsilon, delta)\n",
        "            obs_predict = env.reset()[0]\n",
        "            obs_predict = adversarial_obs.copy()\n",
        "\n",
        "        if randomized_smoothing:\n",
        "            action_total = 0\n",
        "            for i in range(num_smoothed_samples):\n",
        "                noise = np.random.multivariate_normal(np.zeros(obs.shape[0]), lamda*Sigma)\n",
        "                obs_noised = obs_predict + noise\n",
        "                action, _ = model.predict(obs_noised)\n",
        "                action_total += action\n",
        "            if env_name == \"CartPole-v1\":\n",
        "                action = int(round(action_total/num_smoothed_samples,0))\n",
        "            else:\n",
        "                action = action_total/num_smoothed_samples\n",
        "        else:\n",
        "            action, _ = model.predict(obs_predict)\n",
        "\n",
        "        obs, reward, done, _, _ = env.step(action)\n",
        "        if env_name != \"CartPole-v1\":\n",
        "            reward_sum += reward\n",
        "        num_steps += 1\n",
        "        if num_steps >= max_steps or done:\n",
        "            terminated = True\n",
        "\n",
        "    if env_name == \"CartPole-v1\":\n",
        "        return num_steps\n",
        "    else:\n",
        "        return reward_sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lLGwq4X-bd4a"
      },
      "outputs": [],
      "source": [
        "# Generate Samples to Compute Vector v of Value Ranges at each Dimension\n",
        "model = PPO.load(f\"{model_dir}/{'PPO'}_{num_epochs*num_iters}\")\n",
        "env = gym.make(env_name)\n",
        "obs = env.reset()[0]\n",
        "action, _ = model.predict(obs)\n",
        "num_trajectories = 100\n",
        "iterations_per_trajectory = 2500\n",
        "recorded_obs = np.zeros((num_trajectories*iterations_per_trajectory, obs.shape[0]))\n",
        "if env_name == \"CartPole-v1\":\n",
        "    recorded_action = np.zeros((num_trajectories*iterations_per_trajectory))\n",
        "else:\n",
        "    recorded_action = np.zeros((num_trajectories*iterations_per_trajectory, action.shape[0]))\n",
        "for i in range(num_trajectories):\n",
        "    obs = env.reset()[0]\n",
        "    for j in range(iterations_per_trajectory):\n",
        "        action, _ = model.predict(obs)\n",
        "        obs = obs.reshape((1, obs.shape[0]))\n",
        "        obs, _, done, _, _ = env.step(action)\n",
        "\n",
        "        recorded_obs[i*iterations_per_trajectory+j] = obs.copy()\n",
        "        recorded_action[i*iterations_per_trajectory+j] = action.copy()\n",
        "\n",
        "        if done:\n",
        "            obs = env.reset()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3I8CNTOYz7Vk"
      },
      "outputs": [],
      "source": [
        "high_percentile_value = 99\n",
        "low_percentile_value = 1\n",
        "percentile_high = np.percentile(recorded_obs, high_percentile_value, axis=0)\n",
        "percentile_low = np.percentile(recorded_obs, low_percentile_value, axis=0)\n",
        "v = percentile_high - percentile_low\n",
        "Sigma = np.cov(recorded_obs.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1VkKWlNll2l",
        "outputId": "bd06dc9c-1890-4546-9c2e-ad9a688673d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Creating environment from the given name 'HalfCheetah-v4'\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Reward for Naive Model Performance is -24.167888357498498\n"
          ]
        }
      ],
      "source": [
        "# Test Naive Model\n",
        "naive_model = PPO('MlpPolicy', env_name, verbose=1, device='cpu', tensorboard_log=log_dir)\n",
        "result = test(naive_model, env_name, index, num_test_steps)\n",
        "\n",
        "print(\"Reward for Naive Model Performance is \" + str(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqqXTeSJwv-C",
        "outputId": "3eca0288-364a-41c9-b297-d8c352ceba4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Reward for Baseline Performance is 57.51997872265183\n"
          ]
        }
      ],
      "source": [
        "# Test Baseline\n",
        "model = PPO.load(f\"{model_dir}/{'PPO'}_{num_epochs*num_iters}\")\n",
        "result = test(model, env_name, index, num_test_steps)\n",
        "\n",
        "print(\"Reward for Baseline Performance is \" + str(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0a3R2MIln98",
        "outputId": "8d0b2c75-d866-4e4f-9218-854676b1b636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Reward for Adversarially Attacked Performance Using Start State with FGSM is 40.205875310216854\n"
          ]
        }
      ],
      "source": [
        "# Test Adversarial Start State FGSM\n",
        "model = PPO.load(f\"{model_dir}/{'PPO'}_{num_epochs*num_iters}\")\n",
        "result = test(model, env_name, index, num_test_steps, adversarial_attack=\"Start State\", adversarial_mode=\"FGSM\", v=v, epsilon=0.3)\n",
        "\n",
        "print(\"Reward for Adversarially Attacked Performance Using Start State with FGSM is \" + str(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJ9fysTtALl_",
        "outputId": "42626d13-7d7c-4c26-b48c-54055ff0ef1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Reward for Adversarially Attacked Performance Using Start State with PGD is 17.363172384901354\n"
          ]
        }
      ],
      "source": [
        "# Test Adversarial Start State PGD\n",
        "model = PPO.load(f\"{model_dir}/{'PPO'}_{num_epochs*num_iters}\")\n",
        "result = test(model, env_name, index, num_test_steps, adversarial_attack=\"Start State\", adversarial_mode=\"PGD\", v=v, epsilon=0.3)\n",
        "\n",
        "print(\"Reward for Adversarially Attacked Performance Using Start State with PGD is \" + str(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8BSMLFN5Slm",
        "outputId": "aeade17d-e8b8-4ebb-e18d-ac73cd8254e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Reward for Adversarially Attacked Performance Using Observation Corruption with FGSM is -8.19511689431128\n"
          ]
        }
      ],
      "source": [
        "# Test Adversarial Observation Corruption - FGSM\n",
        "model = PPO.load(f\"{model_dir}/{'PPO'}_{num_epochs*num_iters}\")\n",
        "result = test(model, env_name, index, num_test_steps, adversarial_attack=\"Observation Corruption\", adversarial_mode=\"FGSM\", v=v, epsilon=0.3)\n",
        "\n",
        "print(\"Reward for Adversarially Attacked Performance Using Observation Corruption with FGSM is \" + str(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnH-DbMDBt3h",
        "outputId": "25a41eec-6c0a-4ae7-9254-6d92fd161d8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Reward for Adversarially Attacked Performance Using Observation Corruption with PGD is -35.31177856081131\n"
          ]
        }
      ],
      "source": [
        "# Test Adversarial Observation Corruption - PGD\n",
        "model = PPO.load(f\"{model_dir}/{'PPO'}_{num_epochs*num_iters}\")\n",
        "result = test(model, env_name, index, num_test_steps, adversarial_attack=\"Observation Corruption\", adversarial_mode=\"PGD\", v=v, epsilon=0.3)\n",
        "\n",
        "print(\"Reward for Adversarially Attacked Performance Using Observation Corruption with PGD is \" + str(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1HgTxizjIUs",
        "outputId": "2e70dc77-5397-4236-8ca0-d5d1b46ca828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Reward for Baseline Performance with Smoothing is 68.62891560832252\n"
          ]
        }
      ],
      "source": [
        "# Test Baseline Smoothed\n",
        "model = PPO.load(f\"{model_dir}/{'PPO'}_{num_epochs*num_iters}\")\n",
        "result = test(model, env_name, index, num_test_steps, randomized_smoothing=True, Sigma=Sigma)\n",
        "\n",
        "print(\"Reward for Baseline Performance with Smoothing is \" + str(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo7Z2I_MnTGx",
        "outputId": "0979be0c-1be3-4ec1-9ae7-f997e5582a1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Reward for Adversarially Attacked and Smoothed Performance Using Start State with FGSM is 57.844125105432376\n"
          ]
        }
      ],
      "source": [
        "# Test Adversarial Start State with Smoothing - FGSM\n",
        "model = PPO.load(f\"{model_dir}/{'PPO'}_{num_epochs*num_iters}\")\n",
        "result = test(model, env_name, index, num_test_steps, randomized_smoothing=True, Sigma=Sigma, epsilon=0.3, adversarial_attack=\"Start State\", adversarial_mode=\"FGSM\", v=v)\n",
        "\n",
        "print(\"Reward for Adversarially Attacked and Smoothed Performance Using Start State with FGSM is \" + str(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9FleGgAZT8H",
        "outputId": "78fe2cba-a00a-4347-abda-d47739381e9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Reward for Adversarially Attacked and Smoothed Performance Using Start State with PGD is 59.348110201208314\n"
          ]
        }
      ],
      "source": [
        "# Test Adversarial Start State with Smoothing - PGD\n",
        "model = PPO.load(f\"{model_dir}/{'PPO'}_{num_epochs*num_iters}\")\n",
        "result = test(model, env_name, index, num_test_steps, randomized_smoothing=True, Sigma=Sigma, epsilon=0.3, adversarial_attack=\"Start State\", adversarial_mode=\"PGD\", v=v)\n",
        "\n",
        "print(\"Reward for Adversarially Attacked and Smoothed Performance Using Start State with PGD is \" + str(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tUJ86uWS3kk",
        "outputId": "01576c55-82ab-46bc-feec-542239d0b6bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Reward for Adversarially Attacked and Smoothed Performance Using Observation Corruption with FGSM is -1.4928626761952328\n"
          ]
        }
      ],
      "source": [
        "# Test Adversarial Observation Corruption with Smoothing - FGSM\n",
        "model = PPO.load(f\"{model_dir}/{'PPO'}_{num_epochs*num_iters}\")\n",
        "result = test(model, env_name, index, num_test_steps, randomized_smoothing=True, Sigma=Sigma, epsilon=0.3, adversarial_attack=\"Observation Corruption\", adversarial_mode=\"FGSM\", v=v)\n",
        "\n",
        "print(\"Reward for Adversarially Attacked and Smoothed Performance Using Observation Corruption with FGSM is \" + str(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BymFa_dnZaTN",
        "outputId": "73efa41b-f6be-4fc2-a2d3-ccc9d9d9cdb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "-33.511187574594686\n",
            "Average Reward for Adversarially Attacked and Smoothed Performance Using Observation Corruption with PGD is -33.511187574594686\n"
          ]
        }
      ],
      "source": [
        "# Test Adversarial Observation Corruption with Smoothing - PGD\n",
        "model = PPO.load(f\"{model_dir}/{'PPO'}_{num_epochs*num_iters}\")\n",
        "result = test(model, env_name, index, num_test_steps, randomized_smoothing=True, Sigma=Sigma, epsilon=0.3, adversarial_attack=\"Observation Corruption\", adversarial_mode=\"PGD\", v=v)\n",
        "\n",
        "print(\"Reward for Adversarially Attacked and Smoothed Performance Using Observation Corruption with PGD is \" + str(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Dhvv_NLQYVly"
      },
      "outputs": [],
      "source": [
        "if env_name == \"HalfCheetah-v4\":\n",
        "    observation_dimension = 17\n",
        "    action_dimension = 6\n",
        "else:\n",
        "    observation_dimension = 376\n",
        "    action_dimension = 17\n",
        "\n",
        "hidden_size = 64\n",
        "\n",
        "class CloneModel(nn.Module):\n",
        "    def __init__(self, observation_dimension, hidden_size, action_dimension):\n",
        "        super(CloneModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(observation_dimension, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, action_dimension)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize neural network\n",
        "clone_model = CloneModel(observation_dimension, hidden_size, action_dimension)\n",
        "\n",
        "# Define Mean Squared Error (MSE) loss\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Define optimizer (e.g., stochastic gradient descent)\n",
        "optimizer = optim.Adam(clone_model.parameters(), lr=1e-3)\n",
        "\n",
        "# Portion of Data Used As Training (Remaining is Used as Test)\n",
        "training_portion = 0.9\n",
        "num_samples = recorded_obs.shape[0]\n",
        "num_training_samples = int(num_samples*training_portion)\n",
        "num_test_samples = num_samples - num_training_samples\n",
        "\n",
        "# Generate some dummy data for training\n",
        "x_train = torch.tensor(recorded_obs[:num_training_samples]).float()\n",
        "y_train = torch.tensor(recorded_action[:num_training_samples]).float()\n",
        "x_test = torch.tensor(recorded_obs[num_training_samples:]).float()\n",
        "y_test = torch.tensor(recorded_action[num_training_samples:]).float()\n",
        "\n",
        "# Create a TensorDataset\n",
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "test_dataset = TensorDataset(x_test, y_test)\n",
        "\n",
        "# Create a DataLoader for minibatch training\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bVzb4S_cLIU",
        "outputId": "f9d19d15-7d65-4728-b099-6b19a29326d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Loss: 0.0417\n",
            "Epoch [1/100], Test Loss: 0.0276\n",
            "Epoch [2/100], Train Loss: 0.0259\n",
            "Epoch [2/100], Test Loss: 0.0251\n",
            "Epoch [3/100], Train Loss: 0.0240\n",
            "Epoch [3/100], Test Loss: 0.0234\n",
            "Epoch [4/100], Train Loss: 0.0229\n",
            "Epoch [4/100], Test Loss: 0.0228\n",
            "Epoch [5/100], Train Loss: 0.0223\n",
            "Epoch [5/100], Test Loss: 0.0220\n",
            "Epoch [6/100], Train Loss: 0.0218\n",
            "Epoch [6/100], Test Loss: 0.0213\n",
            "Epoch [7/100], Train Loss: 0.0213\n",
            "Epoch [7/100], Test Loss: 0.0207\n",
            "Epoch [8/100], Train Loss: 0.0210\n",
            "Epoch [8/100], Test Loss: 0.0207\n",
            "Epoch [9/100], Train Loss: 0.0208\n",
            "Epoch [9/100], Test Loss: 0.0206\n",
            "Epoch [10/100], Train Loss: 0.0206\n",
            "Epoch [10/100], Test Loss: 0.0201\n",
            "Epoch [11/100], Train Loss: 0.0204\n",
            "Epoch [11/100], Test Loss: 0.0199\n",
            "Epoch [12/100], Train Loss: 0.0203\n",
            "Epoch [12/100], Test Loss: 0.0202\n",
            "Epoch [13/100], Train Loss: 0.0201\n",
            "Epoch [13/100], Test Loss: 0.0199\n",
            "Epoch [14/100], Train Loss: 0.0200\n",
            "Epoch [14/100], Test Loss: 0.0203\n",
            "Epoch [15/100], Train Loss: 0.0199\n",
            "Epoch [15/100], Test Loss: 0.0197\n",
            "Epoch [16/100], Train Loss: 0.0199\n",
            "Epoch [16/100], Test Loss: 0.0201\n",
            "Epoch [17/100], Train Loss: 0.0198\n",
            "Epoch [17/100], Test Loss: 0.0202\n",
            "Epoch [18/100], Train Loss: 0.0197\n",
            "Epoch [18/100], Test Loss: 0.0198\n",
            "Epoch [19/100], Train Loss: 0.0196\n",
            "Epoch [19/100], Test Loss: 0.0195\n",
            "Epoch [20/100], Train Loss: 0.0196\n",
            "Epoch [20/100], Test Loss: 0.0195\n",
            "Epoch [21/100], Train Loss: 0.0195\n",
            "Epoch [21/100], Test Loss: 0.0191\n",
            "Epoch [22/100], Train Loss: 0.0194\n",
            "Epoch [22/100], Test Loss: 0.0197\n",
            "Epoch [23/100], Train Loss: 0.0194\n",
            "Epoch [23/100], Test Loss: 0.0199\n",
            "Epoch [24/100], Train Loss: 0.0193\n",
            "Epoch [24/100], Test Loss: 0.0193\n",
            "Epoch [25/100], Train Loss: 0.0193\n",
            "Epoch [25/100], Test Loss: 0.0192\n",
            "Epoch [26/100], Train Loss: 0.0192\n",
            "Epoch [26/100], Test Loss: 0.0196\n",
            "Epoch [27/100], Train Loss: 0.0192\n",
            "Epoch [27/100], Test Loss: 0.0189\n",
            "Epoch [28/100], Train Loss: 0.0192\n",
            "Epoch [28/100], Test Loss: 0.0193\n",
            "Epoch [29/100], Train Loss: 0.0191\n",
            "Epoch [29/100], Test Loss: 0.0190\n",
            "Epoch [30/100], Train Loss: 0.0191\n",
            "Epoch [30/100], Test Loss: 0.0188\n",
            "Epoch [31/100], Train Loss: 0.0190\n",
            "Epoch [31/100], Test Loss: 0.0192\n",
            "Epoch [32/100], Train Loss: 0.0190\n",
            "Epoch [32/100], Test Loss: 0.0194\n",
            "Epoch [33/100], Train Loss: 0.0190\n",
            "Epoch [33/100], Test Loss: 0.0188\n",
            "Epoch [34/100], Train Loss: 0.0189\n",
            "Epoch [34/100], Test Loss: 0.0193\n",
            "Epoch [35/100], Train Loss: 0.0189\n",
            "Epoch [35/100], Test Loss: 0.0191\n",
            "Epoch [36/100], Train Loss: 0.0189\n",
            "Epoch [36/100], Test Loss: 0.0185\n",
            "Epoch [37/100], Train Loss: 0.0188\n",
            "Epoch [37/100], Test Loss: 0.0189\n",
            "Epoch [38/100], Train Loss: 0.0188\n",
            "Epoch [38/100], Test Loss: 0.0191\n",
            "Epoch [39/100], Train Loss: 0.0188\n",
            "Epoch [39/100], Test Loss: 0.0189\n",
            "Epoch [40/100], Train Loss: 0.0188\n",
            "Epoch [40/100], Test Loss: 0.0190\n",
            "Epoch [41/100], Train Loss: 0.0187\n",
            "Epoch [41/100], Test Loss: 0.0186\n",
            "Epoch [42/100], Train Loss: 0.0187\n",
            "Epoch [42/100], Test Loss: 0.0190\n",
            "Epoch [43/100], Train Loss: 0.0187\n",
            "Epoch [43/100], Test Loss: 0.0189\n",
            "Epoch [44/100], Train Loss: 0.0187\n",
            "Epoch [44/100], Test Loss: 0.0188\n",
            "Epoch [45/100], Train Loss: 0.0187\n",
            "Epoch [45/100], Test Loss: 0.0185\n",
            "Epoch [46/100], Train Loss: 0.0186\n",
            "Epoch [46/100], Test Loss: 0.0189\n",
            "Epoch [47/100], Train Loss: 0.0186\n",
            "Epoch [47/100], Test Loss: 0.0184\n",
            "Epoch [48/100], Train Loss: 0.0186\n",
            "Epoch [48/100], Test Loss: 0.0189\n",
            "Epoch [49/100], Train Loss: 0.0186\n",
            "Epoch [49/100], Test Loss: 0.0185\n",
            "Epoch [50/100], Train Loss: 0.0186\n",
            "Epoch [50/100], Test Loss: 0.0188\n",
            "Epoch [51/100], Train Loss: 0.0185\n",
            "Epoch [51/100], Test Loss: 0.0186\n",
            "Epoch [52/100], Train Loss: 0.0185\n",
            "Epoch [52/100], Test Loss: 0.0182\n",
            "Epoch [53/100], Train Loss: 0.0185\n",
            "Epoch [53/100], Test Loss: 0.0185\n",
            "Epoch [54/100], Train Loss: 0.0185\n",
            "Epoch [54/100], Test Loss: 0.0185\n",
            "Epoch [55/100], Train Loss: 0.0185\n",
            "Epoch [55/100], Test Loss: 0.0185\n",
            "Epoch [56/100], Train Loss: 0.0185\n",
            "Epoch [56/100], Test Loss: 0.0190\n",
            "Epoch [57/100], Train Loss: 0.0184\n",
            "Epoch [57/100], Test Loss: 0.0186\n",
            "Epoch [58/100], Train Loss: 0.0184\n",
            "Epoch [58/100], Test Loss: 0.0186\n",
            "Epoch [59/100], Train Loss: 0.0184\n",
            "Epoch [59/100], Test Loss: 0.0188\n",
            "Epoch [60/100], Train Loss: 0.0184\n",
            "Epoch [60/100], Test Loss: 0.0182\n",
            "Epoch [61/100], Train Loss: 0.0184\n",
            "Epoch [61/100], Test Loss: 0.0183\n",
            "Epoch [62/100], Train Loss: 0.0184\n",
            "Epoch [62/100], Test Loss: 0.0184\n",
            "Epoch [63/100], Train Loss: 0.0184\n",
            "Epoch [63/100], Test Loss: 0.0186\n",
            "Epoch [64/100], Train Loss: 0.0184\n",
            "Epoch [64/100], Test Loss: 0.0184\n",
            "Epoch [65/100], Train Loss: 0.0184\n",
            "Epoch [65/100], Test Loss: 0.0190\n",
            "Epoch [66/100], Train Loss: 0.0184\n",
            "Epoch [66/100], Test Loss: 0.0185\n",
            "Epoch [67/100], Train Loss: 0.0183\n",
            "Epoch [67/100], Test Loss: 0.0182\n",
            "Epoch [68/100], Train Loss: 0.0183\n",
            "Epoch [68/100], Test Loss: 0.0186\n",
            "Epoch [69/100], Train Loss: 0.0183\n",
            "Epoch [69/100], Test Loss: 0.0193\n",
            "Epoch [70/100], Train Loss: 0.0183\n",
            "Epoch [70/100], Test Loss: 0.0185\n",
            "Epoch [71/100], Train Loss: 0.0183\n",
            "Epoch [71/100], Test Loss: 0.0188\n",
            "Epoch [72/100], Train Loss: 0.0183\n",
            "Epoch [72/100], Test Loss: 0.0185\n",
            "Epoch [73/100], Train Loss: 0.0183\n",
            "Epoch [73/100], Test Loss: 0.0185\n",
            "Epoch [74/100], Train Loss: 0.0183\n",
            "Epoch [74/100], Test Loss: 0.0182\n",
            "Epoch [75/100], Train Loss: 0.0183\n",
            "Epoch [75/100], Test Loss: 0.0185\n",
            "Epoch [76/100], Train Loss: 0.0183\n",
            "Epoch [76/100], Test Loss: 0.0188\n",
            "Epoch [77/100], Train Loss: 0.0182\n",
            "Epoch [77/100], Test Loss: 0.0186\n",
            "Epoch [78/100], Train Loss: 0.0182\n",
            "Epoch [78/100], Test Loss: 0.0182\n",
            "Epoch [79/100], Train Loss: 0.0182\n",
            "Epoch [79/100], Test Loss: 0.0188\n",
            "Epoch [80/100], Train Loss: 0.0182\n",
            "Epoch [80/100], Test Loss: 0.0184\n",
            "Epoch [81/100], Train Loss: 0.0182\n",
            "Epoch [81/100], Test Loss: 0.0183\n",
            "Epoch [82/100], Train Loss: 0.0182\n",
            "Epoch [82/100], Test Loss: 0.0186\n",
            "Epoch [83/100], Train Loss: 0.0182\n",
            "Epoch [83/100], Test Loss: 0.0181\n",
            "Epoch [84/100], Train Loss: 0.0182\n",
            "Epoch [84/100], Test Loss: 0.0186\n",
            "Epoch [85/100], Train Loss: 0.0182\n",
            "Epoch [85/100], Test Loss: 0.0187\n",
            "Epoch [86/100], Train Loss: 0.0182\n",
            "Epoch [86/100], Test Loss: 0.0183\n",
            "Epoch [87/100], Train Loss: 0.0182\n",
            "Epoch [87/100], Test Loss: 0.0186\n",
            "Epoch [88/100], Train Loss: 0.0182\n",
            "Epoch [88/100], Test Loss: 0.0184\n",
            "Epoch [89/100], Train Loss: 0.0182\n",
            "Epoch [89/100], Test Loss: 0.0184\n",
            "Epoch [90/100], Train Loss: 0.0181\n",
            "Epoch [90/100], Test Loss: 0.0185\n",
            "Epoch [91/100], Train Loss: 0.0181\n",
            "Epoch [91/100], Test Loss: 0.0184\n",
            "Epoch [92/100], Train Loss: 0.0181\n",
            "Epoch [92/100], Test Loss: 0.0181\n"
          ]
        }
      ],
      "source": [
        "# Train the neural network\n",
        "num_epochs_clone = 100\n",
        "for epoch in range(num_epochs_clone):\n",
        "    running_train_loss = 0.0\n",
        "    for inputs, targets in train_loader:\n",
        "        # Forward pass\n",
        "        outputs = clone_model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    # Print progress\n",
        "    train_loss = running_train_loss / len(train_dataset)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs_clone}], Train Loss: {train_loss:.4f}')\n",
        "\n",
        "    running_test_loss = 0.0\n",
        "    for inputs, targets in test_loader:\n",
        "        outputs = clone_model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        running_test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    test_loss = running_test_loss / len(test_dataset)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs_clone}], Test Loss: {test_loss:.4f}')\n",
        "\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "7PyHX4hRMkgH"
      },
      "outputs": [],
      "source": [
        "def numerical_grad_of_reward_wrt_obs_black_box(obs, model, env, rollout_len=10, delta=1e-4):\n",
        "    grad = np.zeros(obs.shape)\n",
        "    for i in range(obs.shape[0]):\n",
        "        obs_plus = obs.copy()\n",
        "        obs_minus = obs.copy()\n",
        "        obs_plus[i] += delta\n",
        "        obs_minus[i] -= delta\n",
        "\n",
        "        action_plus = model(torch.tensor(obs_plus).float()).detach().numpy()\n",
        "        base_env = deepcopy(env)\n",
        "        result = env.step(action_plus)\n",
        "        obs_new_plus = result[0]\n",
        "        reward_plus = result[1]\n",
        "        for j in range(rollout_len - 1):\n",
        "            action_plus = model(torch.tensor(obs_new_plus).float()).detach().numpy()\n",
        "            result = env.step(action_plus)\n",
        "            obs_new_plus = result[0]\n",
        "            reward_plus += result[1]\n",
        "        reward_plus = reward_plus / rollout_len\n",
        "        env = deepcopy(base_env)\n",
        "\n",
        "        action_minus = model(torch.tensor(obs_minus).float()).detach().numpy()\n",
        "        base_env = deepcopy(env)\n",
        "        result = env.step(action_minus)\n",
        "        obs_new_minus = result[0]\n",
        "        reward_minus = result[1]\n",
        "        for j in range(rollout_len - 1):\n",
        "            action_minus = model(torch.tensor(obs_new_minus).float()).detach().numpy()\n",
        "            result = env.step(action_minus)\n",
        "            obs_new_minus = result[0]\n",
        "            reward_minus += result[1]\n",
        "        reward_minus = reward_minus / rollout_len\n",
        "        env = deepcopy(base_env)\n",
        "\n",
        "        grad[i] = (reward_plus - reward_minus) / (2*delta)\n",
        "\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "YeHN3zKeqx3r"
      },
      "outputs": [],
      "source": [
        "def projected_gradient_descent_black_box(obs, clone_model, env, v, num_attack_iters=10, rollout_len=10, epsilon=0.05, alpha=3e-7, delta=1e-4):\n",
        "    original_obs = obs.copy()\n",
        "    for i in range(num_attack_iters):\n",
        "        grad = numerical_grad_of_reward_wrt_obs_black_box(obs, clone_model, env, rollout_len, delta)\n",
        "        obs = obs - alpha * grad\n",
        "        clipped_obs = np.clip(obs, original_obs - v*epsilon, original_obs + v*epsilon)\n",
        "    return clipped_obs\n",
        "\n",
        "def fgsm_black_box(obs, clone_model, env, v, rollout_len=10, epsilon=0.05, delta=1e-4):\n",
        "    signed_grad = np.sign(numerical_grad_of_reward_wrt_obs_black_box(obs, clone_model, env, rollout_len, delta))\n",
        "    scaled_grad = signed_grad * v * epsilon\n",
        "    obs = obs - scaled_grad\n",
        "    return obs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XW6i4Ae_oXje"
      },
      "outputs": [],
      "source": [
        "def test_black_box(clone_model, black_box_model, env_name, index, max_steps, randomized_smoothing=False, lamda=0.001, Sigma=None, num_smoothed_samples=10, adversarial_attack=None,\n",
        "         adversarial_mode=None, v=None, rollout_len=10, epsilon=0.05, alpha=3e-7, delta=1e-4, num_attack_iters=10):\n",
        "\n",
        "    env = gym.make(env_name)\n",
        "    obs = env.reset()[0]\n",
        "    reward_sum = 0\n",
        "    num_steps = 0\n",
        "    black_box_model.set_env(env)\n",
        "\n",
        "    if v is None:\n",
        "        v = np.ones(obs.shape[0])\n",
        "    if Sigma is None:\n",
        "        Sigma = np.eye(obs.shape[0])\n",
        "\n",
        "    if adversarial_attack == \"Start State\":\n",
        "        if adversarial_mode == \"PGD\":\n",
        "            adversarial_obs = projected_gradient_descent_black_box(obs, clone_model, env, v, num_attack_iters, rollout_len, epsilon, alpha, delta)\n",
        "        else:\n",
        "            adversarial_obs = fgsm_black_box(obs, clone_model, env, v, rollout_len, epsilon, delta)\n",
        "        obs = env.reset()[0]\n",
        "        obs = adversarial_obs.copy()\n",
        "\n",
        "    terminated = False\n",
        "    while not terminated:\n",
        "        obs_predict = obs.copy()\n",
        "        action, _ = black_box_model.predict(obs_predict)\n",
        "\n",
        "        obs, reward, done, _, _ = env.step(action)\n",
        "        reward_sum += reward\n",
        "        num_steps += 1\n",
        "        if num_steps >= max_steps or done:\n",
        "            terminated = True\n",
        "\n",
        "    return reward_sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhyubyQqtnwE",
        "outputId": "1c225023-7255-4fd9-ce8f-5111bc2d1a47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Reward for Black Box Adversarially Attacked Performance Using Start State with FGSM is 48.29666714369076\n"
          ]
        }
      ],
      "source": [
        "# Test Black Box Adversarial Start State FGSM\n",
        "black_box_model = PPO.load(f\"{model_dir}/{'PPO'}_{num_epochs*num_iters}\")\n",
        "result = test_black_box(clone_model, black_box_model, env_name, index, num_test_steps, adversarial_attack=\"Start State\", adversarial_mode=\"FGSM\", v=v, epsilon=0.3)\n",
        "\n",
        "print(\"Reward for Black Box Adversarially Attacked Performance Using Start State with FGSM is \" + str(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWfAIABMt2NT",
        "outputId": "d6dae0cb-27f4-431a-f517-9682eede341d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Reward for Black Box Adversarially Attacked Performance Using Start State with PGD is 38.48371348941361\n"
          ]
        }
      ],
      "source": [
        "# Test Black Box Adversarial Start State PGD\n",
        "black_box_model = PPO.load(f\"{model_dir}/{'PPO'}_{num_epochs*num_iters}\")\n",
        "result = test_black_box(clone_model, black_box_model, env_name, index, num_test_steps, adversarial_attack=\"Start State\", adversarial_mode=\"PGD\", v=v, epsilon=0.3)\n",
        "\n",
        "print(\"Reward for Black Box Adversarially Attacked Performance Using Start State with PGD is \" + str(result))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}